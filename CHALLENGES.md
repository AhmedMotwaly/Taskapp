# üèÜ AutoBuy Guard - Challenges Overcome

**Technical Problem-Solving Journey**

> This document chronicles the major technical challenges encountered during the development of AutoBuy Guard and how they were resolved. These stories demonstrate real-world problem-solving skills that can't be learned from tutorials alone.

---

## üìã Table of Contents

- [Challenge 1: Next.js SSR Hydration Errors](#challenge-1-nextjs-ssr-hydration-errors)
- [Challenge 2: OAuth2 Domain Mismatch](#challenge-2-oauth2-domain-mismatch)
- [Challenge 3: Auto Scaling Health Check Failures](#challenge-4-auto-scaling-health-check-failures)
- [Challenge 4: GitHub Actions Submodule Issues](#challenge-5-github-actions-submodule-issues)
- [Challenge 5: NAT Gateway Cost Optimization](#challenge-6-nat-gateway-cost-optimization)
- [Challenge 6: Zalando Variant Detection](#challenge-7-zalando-variant-detection)
- [Key Lessons Learned](#key-lessons-learned)

---

## üî¥ Challenge 1: Next.js SSR Hydration Errors

### The Problem

**Error Message:**
```
Error: Hydration failed because the initial UI does not match what was rendered on the server.
ReferenceError: localStorage is not defined
```

**Context:**  
While building the dashboard to display tracked items, I stored user preferences in localStorage. The code worked perfectly in development but crashed in production.

**The Failing Code:**
```javascript
// Dashboard.tsx
import { useState, useEffect } from 'react';

export default function Dashboard() {
  const [items, setItems] = useState([]);
  
  useEffect(() => {
    const saved = localStorage.getItem('tracked-items');
    if (saved) {
      setItems(JSON.parse(saved));
    }
  }, []);

  return <div>{/* Display items */}</div>;
}
```

**Why It Failed:**  
Next.js 14 App Router uses React Server Components by default. Server components render on the server (Node.js environment) where browser APIs like `localStorage` don't exist. The server rendered empty content, but the client tried to render with localStorage data, causing a hydration mismatch.

### The Investigation

**Step 1: Read the Error Stack Trace**
```
Warning: Expected server HTML to contain a matching <div> in <div>.
    at Dashboard
    at page
```

This told me the mismatch was in the Dashboard component.

**Step 2: Research the Error**  
Found that Next.js 13+ separates Server Components (default) from Client Components (marked with `'use client'`).

**Step 3: Test Hypothesis**  
Tried wrapping the component with `'use client'` directive:

```javascript
'use client';  // Added this line

export default function Dashboard() {
  // ... same code
}
```

**Result:** Error disappeared! But this felt like a hack without understanding *why*.

### The Solution

**Final Implementation:**
```typescript
// app/dashboard/page.tsx (Server Component)
export default function DashboardPage() {
  // Server component - no localStorage access
  return <DashboardClient />;
}

// components/DashboardClient.tsx (Client Component)
'use client';

import { useState, useEffect } from 'react';

export default function DashboardClient() {
  const [items, setItems] = useState([]);
  const [isLoading, setIsLoading] = useState(true);
  
  useEffect(() => {
    // Safe to use browser APIs here
    const saved = localStorage.getItem('tracked-items');
    if (saved) {
      setItems(JSON.parse(saved));
    }
    setIsLoading(false);
  }, []);

  if (isLoading) {
    return <div>Loading...</div>;
  }

  return <div>{/* Display items */}</div>;
}
```

**Why This Works:**
- **Server Component** (page.tsx) renders basic shell on server
- **Client Component** wraps interactive parts that need browser APIs
- No hydration mismatch because client component only renders on client
- Better performance: Server components are lighter weight

### What I Learned

1. **Next.js App Router fundamentals**: Server Components vs Client Components
2. **When to use each**:
   - Server Components: Data fetching, static content
   - Client Components: Interactivity, browser APIs, state
3. **Hydration errors indicate server/client mismatch**: Always check what's rendering differently between server and client
4. **Progressive enhancement**: Start with server rendering, add client interactivity where needed

**Time to Debug:** 4 hours  
**Time Saved Long-Term:** Countless hours by understanding the pattern

---

## üî¥ Challenge 2: OAuth2 Domain Mismatch

### The Problem

**Error Message:**
```
Error 400: redirect_uri_mismatch

The redirect URI in the request: http://autobuyguard.store/api/auth/callback/google
does not match a registered redirect URI.
```

**Context:**  
After deploying to production, Google OAuth login failed. It worked perfectly in development.

**Screenshot of Google Console:**
```
Authorized redirect URIs:
‚úÖ https://www.autobuyguard.store/api/auth/callback/google
‚ùå http://autobuyguard.store/api/auth/callback/google  (missing!)
```

### The Investigation

**Step 1: Compare Development vs Production**

Development (Working):
```bash
NEXTAUTH_URL=http://localhost:3000
```

Production (Failing):
```bash
NEXTAUTH_URL=https://autobuyguard.store  # Sometimes
NEXTAUTH_URL=https://www.autobuyguard.store  # Other times
```

**Step 2: Check All Configuration Points**

Found **5 different places** where domain was configured:

1. **.env file** in local development
2. **Amplify environment variables**
3. **Google Cloud Console** redirect URIs
4. **Route 53 DNS** records
5. **NextAuth.js configuration**

**Problem Identified:**  
Configuration was **inconsistent** - sometimes using `autobuyguard.store`, sometimes `www.autobuyguard.store`, sometimes `http://`, sometimes `https://`.

### The Solution

**Step 1: Standardize on ONE Domain**

Decision: Use `https://www.autobuyguard.store` everywhere (www is standard for web apps).

**Step 2: Update All Configuration Points**

```bash
# 1. Amplify Environment Variables
NEXTAUTH_URL=https://www.autobuyguard.store
NEXTAUTH_SECRET=<secret>
NEXT_PUBLIC_API_URL=https://api.autobuyguard.store

# 2. Google Cloud Console Redirect URIs
https://www.autobuyguard.store/api/auth/callback/google

# 3. NextAuth Configuration
// app/api/auth/[...nextauth]/route.ts
export const authOptions = {
  providers: [GoogleProvider(...)],
  callbacks: {
    redirect: async ({ url, baseUrl }) => {
      // Force www subdomain
      if (url.startsWith('http://autobuyguard.store')) {
        return 'https://www.autobuyguard.store';
      }
      return url.startsWith(baseUrl) ? url : baseUrl;
    }
  }
};
```

**Step 3: Configure Route 53 DNS Properly**

```yaml
# Route 53 Records
A Record:
  Name: autobuyguard.store
  Type: A
  Value: CloudFront distribution

CNAME Record:
  Name: www.autobuyguard.store
  Type: CNAME
  Value: CloudFront distribution

# Redirect non-www to www at CloudFront level
```

**Step 4: Rebuild Amplify App**

Amplify caches environment variables at build time, so needed to trigger a rebuild:

```bash
# Force rebuild to inject updated env vars
git commit --allow-empty -m "Rebuild: Update OAuth URLs"
git push origin main
```

### What I Learned

1. **OAuth is environment-sensitive**: Every URL must match exactly
2. **Configuration sprawl is dangerous**: Document all config locations
3. **Standardize early**: Pick www vs non-www on day 1
4. **Environment variables are build-time**: Amplify bakes them into the build
5. **DNS takes time**: Route 53 changes can take 5-10 minutes to propagate

**Debugging Process:**
- Checked browser console (redirect URL mismatch)
- Checked Google Console (registered URIs)
- Checked Amplify logs (environment variables)
- Checked Route 53 (DNS configuration)

**Time to Debug:** 6 hours (across 2 days)  
**Root Cause:** Configuration inconsistency

---

## üî¥ Challenge 3: Auto Scaling Health Check Failures

### The Problem

**Symptom:**  
After pushing new backend code, GitHub Actions deployed successfully, but the Auto Scaling Group showed:

```
Instance Status: Unhealthy
Health Check: Failing
Target Group: 0 healthy targets
```

**Impact:**  
New instances launched but immediately marked unhealthy. Old instances terminated. **Result: Complete outage for 15 minutes.**

### The Investigation

**Step 1: Check ALB Target Health**

AWS Console ‚Üí Target Groups ‚Üí Targets:
```
Instance: i-abc123
Status: Unhealthy
Health check: HTTP:80/health
Status code: -
Reason: Connection refused
```

**Step 2: SSH Into Instance**

```bash
ssh -i taskapp-key.pem ec2-user@<instance-ip>

# Check Docker container
docker ps
# No containers running!

# Check Docker logs
docker logs taskapp-backend
# Container doesn't exist

# Check User Data logs
cat /var/log/user-data.log
```

**Found the Error:**
```bash
/var/lib/cloud/instance/user-data.txt: line 23: syntax error near unexpected token `}'
```

**Root Cause:** Syntax error in the User Data script. The Docker container never started, so the health check endpoint at `/health` was unreachable.

**The Broken User Data Script:**
```bash
#!/bin/bash

# ... earlier commands ...

# This line had unmatched braces
export DB_PASS=$(echo $SECRETS | jq -r .password}  # ‚Üê Missing opening {
```

### The Solution

**Step 1: Fix User Data Script**

```bash
#!/bin/bash
set -e  # Exit immediately on any error
exec > >(tee /var/log/user-data.log)  # Log everything
exec 2>&1  # Redirect stderr to stdout

echo "=== User Data Script Started at $(date) ==="

# Update system
echo "Updating system packages..."
yum update -y

# Install Docker
echo "Installing Docker..."
yum install docker -y
systemctl start docker
systemctl enable docker

# Install AWS CLI
echo "Installing AWS CLI..."
curl "https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip" -o "awscliv2.zip"
unzip awscliv2.zip
./aws/install

# Retrieve secrets
echo "Fetching secrets from Secrets Manager..."
SECRETS=$(aws secretsmanager get-secret-value \
  --secret-id taskapp/database \
  --region eu-central-1 \
  --query SecretString \
  --output text)

# Parse secrets with proper syntax
echo "Parsing secrets..."
export DB_HOST=$(echo $SECRETS | jq -r .host)
export DB_USER=$(echo $SECRETS | jq -r .username)
export DB_PASS=$(echo $SECRETS | jq -r .password)  # ‚Üê Fixed!
export DB_NAME=$(echo $SECRETS | jq -r .database)

echo "Database config: $DB_HOST:3306/$DB_NAME"

# Login to ECR
echo "Logging into ECR..."
aws ecr get-login-password --region eu-central-1 | \
  docker login --username AWS --password-stdin \
  050752651038.dkr.ecr.eu-central-1.amazonaws.com

# Pull and run Docker container
echo "Pulling Docker image..."
docker pull 050752651038.dkr.ecr.eu-central-1.amazonaws.com/taskapp-backend:latest

echo "Starting Docker container..."
docker run -d \
  -p 80:5000 \
  --restart unless-stopped \
  --name taskapp-backend \
  -e DB_HOST=$DB_HOST \
  -e DB_USER=$DB_USER \
  -e DB_PASS=$DB_PASS \
  -e DB_NAME=$DB_NAME \
  050752651038.dkr.ecr.eu-central-1.amazonaws.com/taskapp-backend:latest

# Wait for container to be ready
echo "Waiting for application to start..."
sleep 10

# Test health endpoint
echo "Testing health endpoint..."
curl -f http://localhost/health || {
  echo "Health check failed!"
  docker logs taskapp-backend
  exit 1
}

echo "=== User Data Script Completed Successfully at $(date) ==="
```

**Key Improvements:**
- `set -e`: Exit on first error (fail fast)
- Comprehensive logging with timestamps
- Test health endpoint before declaring success
- Fixed syntax error in jq parsing

**Step 2: Add Health Check Endpoint Verification**

```javascript
// backend/index.js
app.get('/health', (req, res) => {
  // More robust health check
  const health = {
    status: 'healthy',
    timestamp: Date.now(),
    uptime: process.uptime(),
    database: 'connected'  // Would check DB connection here
  };
  
  res.status(200).json(health);
});
```

**Step 3: Increase Health Check Grace Period**

```yaml
# Auto Scaling Group Configuration
HealthCheckGracePeriod: 300  # Changed from 30s to 300s (5 minutes)
```

**Why 5 Minutes:**
- Docker pull: ~60 seconds
- Container startup: ~30 seconds
- Application initialization: ~20 seconds
- Buffer: ~190 seconds

**Step 4: Add CloudWatch Alarms**

```bash
# Create alarm for unhealthy targets
aws cloudwatch put-metric-alarm \
  --alarm-name taskapp-unhealthy-targets \
  --metric-name UnHealthyHostCount \
  --namespace AWS/ApplicationELB \
  --statistic Average \
  --period 60 \
  --threshold 1 \
  --comparison-operator GreaterThanThreshold
```

### What I Learned

1. **Invisible failures are the hardest to debug**: Without logs, I was blind
2. **Always log User Data scripts**: Use `exec > >(tee /var/log/user-data.log)`
3. **Syntax errors are silent killers**: Scripts fail without notifying you
4. **Test locally first**: Never deploy untested User Data changes
5. **Grace periods matter**: Give instances time to become healthy
6. **Health checks should be robust**: Not just return 200, but verify actual health

**Debugging Tools Used:**
- AWS Console (Target Group health)
- SSH access to instance
- User Data logs (`/var/log/user-data.log`)
- Docker logs (`docker logs`)
- CloudWatch logs

**Time to Debug:** 2 hours (with 15-minute outage)  
**Prevention:** Comprehensive logging saved hours on future issues

---

## üî¥ Challenge 4: GitHub Actions Submodule Issues

### The Problem

**Symptom:**  
After pushing code to GitHub, the backend and frontend folders appeared empty in the repository web UI.

```
Taskapp/
‚îú‚îÄ‚îÄ backend/          (empty - 0 files)
‚îú‚îÄ‚îÄ frontend/         (empty - 0 files)
‚îú‚îÄ‚îÄ .github/
‚îî‚îÄ‚îÄ README.md
```

But locally, both folders had all the files!

### The Investigation

**Step 1: Check Git Status Locally**

```bash
git status
# On branch main
# nothing to commit, working tree clean
```

Everything looked fine locally.

**Step 2: Check GitHub Repository**

Browser ‚Üí github.com/AhmedMotwaly/Taskapp:
- backend folder: Empty
- frontend folder: Empty

**Step 3: Clone Fresh Copy**

```bash
git clone https://github.com/AhmedMotwaly/Taskapp.git test-clone
cd test-clone
ls backend/
# (empty)
```

**Confirmed:** Files weren't being pushed to GitHub.

**Step 4: Check Git Tracking**

```bash
git ls-files | head -20
# backend/
# frontend/
# .gitignore
```

Git was tracking the *folders* but not the *files inside*.

**Step 5: Check for Hidden .git Folders**

```bash
ls -la backend/
# .git/  ‚Üê Found it!
# .gitignore
# index.js
# ...

ls -la frontend/
# .git/  ‚Üê Found this too!
# .gitignore
# package.json
# ...
```

**Root Cause:** Each folder had its own `.git` directory, making Git treat them as **submodules** instead of regular folders.

### The Solution

**Step 1: Remove Submodule Cache**

```bash
# Remove folders from Git's index
git rm --cached backend
git rm --cached frontend

# Commit the removal
git commit -m "Remove submodule cache"
git push origin main
```

**Step 2: Delete Hidden .git Folders**

```bash
# Remove .git from backend
rm -rf backend/.git

# Remove .git from frontend
rm -rf frontend/.git

# Verify they're gone
ls -la backend/ | grep .git
# (no output = success)
```

**Step 3: Rename Folders (Git Cache Reset)**

This forces Git to see them as "new" folders:

```bash
# Rename to temporary names
mv backend backend-temp
mv frontend frontend-temp

# Rename to lowercase (Git sees this as different on case-sensitive systems)
mv backend-temp backend
mv frontend-temp frontend
```

**Step 4: Re-add as Regular Folders**

```bash
# Add everything
git add backend/
git add frontend/

# Check what's staged
git status
# Should show hundreds of new files

# Commit
git commit -m "Fix: Convert submodules to regular folders"

# Force push to overwrite remote
git push origin main --force
```

**Step 5: Verify on GitHub**

Refresh GitHub ‚Üí backend and frontend now show all files!

### What I Learned

1. **Hidden .git folders cause submodule issues**: Always check for nested .git directories
2. **Git submodules are intentional**: They link to external repos, not embed files
3. **git rm --cached is your friend**: Removes from index without deleting local files
4. **git ls-files shows what's tracked**: Use it to verify what Git actually sees
5. **--force push with caution**: Only when you know the remote is wrong

**Red Flags for Submodule Issues:**
- Folders show as single line items (not expandable) on GitHub
- `git clone` creates empty folders
- `git ls-files` doesn't show files inside folders
- Folders have their own `.git` directory

**Time to Debug:** 3 hours  
**Key Lesson:** Don't copy .git folders when moving code between repos

---

## üü° Challenge 5: NAT Gateway Cost Optimization

### The Problem

**Context:**  
During the first month, AWS bill was **$75/month**, exceeding the $50 target budget.

**Cost Breakdown:**
```
EC2:                    $10.15
RDS:                    $9.48
ALB:                    $14.47
NAT Gateway:            $32.85  ‚Üê 43% of total!
Other:                  $8.05
---
Total:                  $75.00
```

**Question:** How to reduce costs without compromising functionality?

### The Investigation

**Step 1: Analyze Cost Explorer**

AWS Cost Explorer ‚Üí Filter by Service:
- NAT Gateway: $32.85/month ($0.045/hour + data transfer)

**Step 2: Understand NAT Gateway Purpose**

NAT Gateway allows instances in private subnets to access the internet while remaining unreachable from the internet.

**Current Architecture:**
```
EC2 (Private Subnet) ‚Üí NAT Gateway ‚Üí Internet Gateway ‚Üí Internet
```

**Step 3: Question the Need**

Do EC2 instances NEED to be in private subnets?

What do they need internet access for:
- Pulling Docker images from ECR
- Accessing Secrets Manager
- Sending CloudWatch logs
- yum update for security patches

**Step 4: Explore Alternatives**

**Option 1:** Keep NAT Gateway (Most Secure)
- Cost: $32/month
- Security: Maximum isolation

**Option 2:** Use VPC Endpoints (Secure + Cheaper)
- Cost: ~$7-15/month (per endpoint)
- Security: Private connections to AWS services
- Issue: Need multiple endpoints (ECR, Secrets Manager, CloudWatch)

**Option 3:** Move EC2 to Public Subnets (Cost-Optimized)
- Cost: $0 (Internet Gateway is free)
- Security: Lower isolation, but manageable with security groups

### The Solution

**Decision:** Move EC2 instances to public subnets (Option 3).

**Reasoning:**
- This is an MVP with low traffic
- Security maintained through other layers:
  - RDS remains in private subnets (most critical)
  - Security groups enforce strict rules
  - TLS encryption on all connections
  - Rate limiting on API
- Can add NAT Gateway or VPC Endpoints later when revenue justifies cost

**Implementation:**

**Step 1: Update Auto Scaling Group**

```bash
aws autoscaling update-auto-scaling-group \
  --auto-scaling-group-name taskapp-backend-asg \
  --vpc-zone-identifier "subnet-public-1a,subnet-public-1b,subnet-public-1c"
```

**Step 2: Verify Security Groups**

```yaml
TaskApp-WebServer-SG:
  Inbound:
    - Type: HTTP, Port: 80, Source: TaskApp-ALB-SG
    - Type: SSH, Port: 22, Source: <my-ip>/32
  Outbound:
    - Type: All Traffic, Destination: 0.0.0.0/0
```

**Step 3: Update Route Table References**

No changes needed - public subnets already route to Internet Gateway.

**Step 4: Terminate NAT Gateway**

```bash
# Find NAT Gateway ID
aws ec2 describe-nat-gateways --filter "Name=vpc-id,Values=vpc-xxx"

# Delete NAT Gateway
aws ec2 delete-nat-gateway --nat-gateway-id nat-xxx

# Release Elastic IP (after NAT Gateway deleted)
aws ec2 release-address --allocation-id eipalloc-xxx
```

**Step 5: Trigger Instance Refresh**

```bash
aws autoscaling start-instance-refresh \
  --auto-scaling-group-name taskapp-backend-asg
```

**Result After 1 Month:**
```
EC2:                    $10.15
RDS:                    $9.48
ALB:                    $14.47
VPC (Data Transfer):    $6.73
Other:                  $1.92
---
Total:                  $42.75  (43% savings!)
```

### Trade-offs & Mitigation

**What We Lost:**
- Network layer isolation for EC2 instances

**What We Maintained:**
- RDS in private subnets (no internet access)
- Security group restrictions
- TLS encryption everywhere
- Rate limiting on API
- IAM roles (no embedded credentials)

**When to Reconsider:**
- Production with paying customers
- Compliance requirements (PCI-DSS, HIPAA)
- High-security requirements
- Can afford $32/month for enhanced isolation

**Alternative Future Path:**
Use VPC Endpoints instead of NAT Gateway:
- Cheaper (~$7/month per endpoint)
- Private connections to AWS services
- Still avoid public internet for EC2

### What I Learned

1. **Cost optimization requires trade-offs**: Perfect security vs. practical budget
2. **Defense in depth still works**: Multiple security layers, not just networking
3. **NAT Gateway is expensive**: Consider alternatives for MVP stage
4. **Document architectural decisions**: Explain the "why" behind trade-offs
5. **Scaling isn't just about capacity**: Sometimes it's about adding security layers

**How to Explain to Recruiters:**

> "I initially designed with a NAT Gateway following AWS best practices. However, when I analyzed costs, NAT Gateway alone was 43% of my bill. I made a conscious decision to move EC2 to public subnets for the MVP stage, while maintaining security through other layers. RDS remains in private subnets, security groups enforce strict rules, and all connections use TLS. This saved $32/month while keeping the application production-ready. For a scaled production system, I'd add NAT Gateway or VPC Endpoints when revenue justifies the cost."

**Time to Decision:** 2 days (research + implementation)  
**Cost Savings:** $32/month (43% reduction)

---

## üü° Challenge 6: Zalando Variant Detection

### The Problem

**Symptom:**  
Users added Zalando product URLs like:
```
https://www.zalando.de/adidas-originals-sneaker-low-white.html
```

But when the scraper checked, it returned:
```javascript
{
  available: false  // Wrong! Item is in stock
}
```

**Context:**  
Zalando products have multiple variants (sizes, colors). A product might be out of stock in size 42 but available in size 44.

### The Investigation

**Manual Test:**
- Size 38: Out of stock
- Size 40: In stock
- Size 42: In stock
- Size 44: Out of stock

**Current Scraper Logic:**
```javascript
const available = await page.$('.add-to-basket-button');
// Returns null if ANY variant is out of stock
```

**Problem:** The scraper only checked if the "Add to Basket" button existed, but Zalando hides the button when the *selected* variant is unavailable, even if other variants are in stock.

### The Solution

**Version 13.1: Variant-Aware Detection**

```javascript
async function scrapeZalando(url) {
  const page = await browser.newPage();
  await page.goto(url, { waitUntil: 'networkidle2' });
  
  // Check if ANY variant is available
  const variantData = await page.evaluate(() => {
    // Find size selector
    const sizeButtons = document.querySelectorAll('[data-testid="size-button"]');
    
    let anyAvailable = false;
    let availableSizes = [];
    
    sizeButtons.forEach(button => {
      const isAvailable = !button.disabled && 
                         !button.classList.contains('is-out-of-stock');
      
      if (isAvailable) {
        anyAvailable = true;
        availableSizes.push(button.textContent.trim());
      }
    });
    
    return {
      available: anyAvailable,
      availableSizes: availableSizes,
      totalVariants: sizeButtons.length
    };
  });
  
  return variantData;
}
```

**Result:**
```javascript
{
  available: true,
  availableSizes: ["40", "42", "43"],
  totalVariants: 8
}
```

### What I Learned

1. **E-commerce is complex**: Products aren't binary in-stock/out-of-stock
2. **Variants require special handling**: Size, color, model all matter
3. **Test with real data**: Use actual product pages, not test data
4. **User experience matters**: Notifying "Size 42 back in stock" is better than "Product in stock"

**Time to Debug:** 8 hours  
**Iteration:** Version 13.1 (took 13 attempts to get variant detection right!)

---

## üéì Key Lessons Learned

### Technical Lessons

1. **Fail fast, log everything**: Silent failures waste hours of debugging
2. **Test in production-like environments**: Development doesn't catch everything
3. **Read error messages carefully**: They usually point to the exact problem
4. **Document architectural decisions**: Explain the "why" behind trade-offs
5. **Iterate based on real data**: Version 13.1 exists because versions 1-12 had edge cases

### Problem-Solving Process

**When Stuck:**
1. Reproduce the error reliably
2. Read the error message / stack trace
3. Search for similar issues (GitHub, Stack Overflow)
4. Form a hypothesis
5. Test the hypothesis
6. If wrong, form a new hypothesis
7. Repeat until solved

**When to Ask for Help:**
- After 2 hours with no progress
- When the problem is completely outside my knowledge domain
- When I've exhausted all documentation

### Soft Skills

1. **Perseverance**: Some bugs took days to solve
2. **Curiosity**: Deep-dive into "why" instead of just "how to fix"
3. **Humility**: Willing to scrap code and start over (Zalando scraper v13.1!)
4. **Documentation**: Write down what worked AND what didn't

### Advice for Others

**Starting a Cloud Project:**
- Start small, add complexity gradually
- Log everything from day 1
- Use version control religiously
- Test in production-like environments
- Budget for mistakes (AWS costs can spike during experimentation)

**Debugging Production Issues:**
- CloudWatch logs are your best friend
- SSH access to instances is invaluable
- Health checks should be comprehensive
- Always have a rollback plan

**Learning Cloud Engineering:**
- Theory is important, but nothing beats building real systems
- Break things, fix them, document what you learned
- Every error is a learning opportunity
- Don't be afraid to ask "why does this work?"

---

## üìä Challenge Statistics

| Challenge | Time to Debug | Severity | Impact |
|-----------|--------------|----------|--------|
| Next.js SSR Errors | 4 hours | Medium | User experience |
| OAuth2 Domain | 6 hours | High | Complete login failure |
| Health Check Failures | 2 hours | Critical | 15-minute outage |
| GitHub Submodules | 3 hours | Low | Development workflow |
| NAT Gateway Cost | 2 days | Medium | Budget overrun |
| Zalando Variants | 8 hours | Medium | False negatives |

**Total Debug Time:** ~45 hours  
**Total Outages:** 1 (15 minutes)  
**Bugs Remaining:** 0 critical, working on edge cases

---

## üéØ What This Demonstrates

These challenges prove:

1. ‚úÖ **Real-world problem solving** - Not tutorial-following
2. ‚úÖ **Debugging skills** - Methodical investigation process
3. ‚úÖ **Architectural thinking** - Cost vs. security trade-offs
4. ‚úÖ **Persistence** - Iterated through 13 versions on Zalando scraper
5. ‚úÖ **Learning agility** - Career changer solving production issues
6. ‚úÖ **Production mindset** - Comprehensive logging, monitoring, rollbacks

**These stories are worth more than certifications in interviews.**

---

**Built with ‚ù§Ô∏è and lots of debugging by Ahmed Motwaly**

*"Every bug solved is a lesson learned."*
